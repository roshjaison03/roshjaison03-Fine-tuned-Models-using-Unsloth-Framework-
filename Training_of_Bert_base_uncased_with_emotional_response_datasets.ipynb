{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roshjaison03/roshjaison03-Fine-tuned-Models-using-Unsloth-Framework-/blob/main/Training_of_Bert_base_uncased_with_emotional_response_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes datasets transformers"
      ],
      "metadata": {
        "id": "UQvFoglahPcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig\n",
        "\n",
        "# Define 4-bit quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # or \"fp4\" if preferred\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "# Load tokenizer and model with BitsAndBytes 4-bit config\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\n",
        "    \"google-bert/bert-base-uncased\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"  # optional: auto place on GPU if available\n",
        ")\n"
      ],
      "metadata": {
        "id": "KnlZ77vlhKwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query\", \"value\", \"key\", \"dense\"],  # Adjust based on your model\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM   # Use this for MLM\n",
        ")\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47PpJv4OvSOQ",
        "outputId": "bea9a7b1-b1de-4d01-99f6-42897bfcfd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV as DataFrame\n",
        "df = pd.read_csv(\"/content/converted_dataset.csv\")\n",
        "\n",
        "def format_row(row):\n",
        "    input_text = f\"{row['input']}\"\n",
        "    label_text = f\"{row['label']}\"\n",
        "    return input_text, label_text\n",
        "\n",
        "df[['input', 'label']] = df.apply(\n",
        "    lambda row: pd.Series(format_row(row)), axis=1)\n",
        "\n",
        "# Preview the result\n",
        "print(df[['input', 'label']].head())\n"
      ],
      "metadata": {
        "id": "uACRS_lQp-D5",
        "outputId": "7d3c906e-9317-4475-c089-2b2e6e96cfb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First input: I have no energy.  The thought of working out now is out of the question.  I used to bike and play tennis but that seems a lifetime ago.  I would love to feel alive again, but I am just so tired all the time.\n",
            "First label: Getting back to regular physical activity is really appealing to you but also seems a bit out of reach. Youâ€™re excited to get back to a place where you feel invigorated again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end   = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = f\"\"\"You have to talk like a person where you are trying to understand emotions of a person understand the cause of it {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your response empathetically {solution_start} and {solution_end}.\"\"\"\n",
        "\n",
        "system_prompt"
      ],
      "metadata": {
        "id": "YDSXqKeuiCDw",
        "outputId": "79a32433-337e-44ac-c3d8-81828164406d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You have to talk like a person where you are trying to understand emotions of a person understand the cause of it <start_working_out> and <end_working_out>.\\nThen, provide your response empathetically <SOLUTION> and </SOLUTION>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.apply(lambda x: {\n",
        "    \"prompt\": [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": x[\"input\"].strip()},\n",
        "    ],\n",
        "    \"answer\": x[\"label\"].strip(\" []',\\\"\\n\")\n",
        "}, axis=1)\n",
        "\n",
        "# Convert to list of dicts (if needed for training)\n",
        "dataset = dataset.tolist()\n",
        "\n",
        "# Preview the first example\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "eF2pv8FVoqBc",
        "outputId": "890457e3-f8b5-4058-a003-8a2e0c31d182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': [{'role': 'system', 'content': 'You have to talk like a person where you are trying to understand emotions of a person understand the cause of it <start_working_out> and <end_working_out>.\\nThen, provide your response empathetically <SOLUTION> and </SOLUTION>.'}, {'role': 'user', 'content': \"I will have to look at that.  I've not been in the job market for 25 years.  I thought they may be able to provide some assistance in just where do I begin. 4\\nI'm a teacher at a private school.  4\"}], 'answer': 'You want to know where to start.  Sounds like you are young.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_list(dataset)\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "eEgzmUer7oQK",
        "outputId": "b91fbcce-ccef-4660-b3f7-32b171866300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': [{'content': 'You have to talk like a person where you are trying to understand emotions of a person understand the cause of it <start_working_out> and <end_working_out>.\\nThen, provide your response empathetically <SOLUTION> and </SOLUTION>.', 'role': 'system'}, {'content': \"I will have to look at that.  I've not been in the job market for 25 years.  I thought they may be able to provide some assistance in just where do I begin. 4\\nI'm a teacher at a private school.  4\", 'role': 'user'}], 'answer': 'You want to know where to start.  Sounds like you are young.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLMDataCollator:\n",
        "    def __init__(self, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        texts = []\n",
        "\n",
        "        for example in batch:\n",
        "            prompt_text = self._format_prompt(example[\"prompt\"])\n",
        "            full_text = f\"{prompt_text}\\nAnswer: {example['answer']}\"\n",
        "            texts.append(full_text)\n",
        "\n",
        "        encodings = self.tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encodings[\"input_ids\"],\n",
        "            \"attention_mask\": encodings[\"attention_mask\"],\n",
        "            \"labels\": encodings[\"input_ids\"].clone()  # For MLM training\n",
        "        }\n",
        "\n",
        "    def _format_prompt(self, prompt_list):\n",
        "        \"\"\"Convert prompt list to formatted string.\"\"\"\n",
        "        formatted_text = \"\"\n",
        "        for message in prompt_list:\n",
        "            role = message[\"role\"]\n",
        "            content = message[\"content\"]\n",
        "            formatted_text += f\"{role.capitalize()}: {content}\\n\"\n",
        "        return formatted_text.strip()\n"
      ],
      "metadata": {
        "id": "nK3a4Z2k0NIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = MLMDataCollator(tokenizer, max_length=1024)"
      ],
      "metadata": {
        "id": "k4ddhIKQ02BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/empathetic_model/results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    logging_dir=\"/content/drive/MyDrive/empathetic_model/logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"no\",\n",
        "    fp16=True,  # if on Colab GPU\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "WR1QZRGKtEWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC6IG1Jiud4C",
        "outputId": "d80f75e4-f156-467a-8107-9ed0e06c43a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tqpaCtmPuldx",
        "outputId": "26dbeb27-811d-455f-9233-a051cf5dfa85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}